\documentclass[a4paper,11pt]{article}

% Language, encoding, and layout
\usepackage{style}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{array}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage[a4paper,margin=2.5cm]{geometry}
\usepackage{setspace}
\usepackage{comment}
\onehalfspacing
\usepackage{parskip} % no indent, space between paragraphs

% Mathematics
\usepackage{amsmath, amssymb, amsthm}
\usepackage{bbm}
\usepackage{mathrsfs}  

% Graphics
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}

% Diagrams
\usepackage{tikz}
\usetikzlibrary{positioning, shapes.geometric, arrows}

% Colors
\usepackage{xcolor}

% Tables
\usepackage{booktabs}

\usepackage{lmodern}  % Modernere Schrift

% Farben definieren
\definecolor{mainblue}{RGB}{0,70,127}
\definecolor{lightgray}{gray}{0.85}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
  pdftitle={The Euler-Bernoulli Beam Equation and PINN},
  pdfauthor={Dr. Robert Haas},
  pdfsubject={Whitepaper on Beam Theory and Neural Networks},
  pdfkeywords={Beam Equation, PINN, Euler-Bernoulli, Mechanics, Neural Networks, PDE},
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

% Bibliography
%\bibliographystyle{plain}  % numerischer Stil
\bibliographystyle{abbrvnat}
% \bibliographystyle{plainnat}
% \bibliography{literatur}   % Datei literatur.bib

\usepackage{hyperref} % Für klickbare Links im PDF

% Header and footer

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Beam Equation and PINN}
\fancyhead[R]{\thepage}

%\fancyhead[L]{\textbf{Beam Equation and PINN}}
%\fancyhead[R]{Dr. Robert Haas}
%\renewcommand{\headrulewidth}{0.4pt}

\fancyfoot[L]{\textcolor{gray}{\small Robert Haas – Technical Software and Mathematics}}
\fancyfoot[R]{\textcolor{gray}{\small Page \thepage\ of \pageref{LastPage}}}
\renewcommand{\footrulewidth}{0.4pt}

% --- Für \pageref{LastPage} ---
\usepackage{lastpage}

\def\Meter{\mathrm{m}}
\def\Millimeter{\mathrm{mm}}
\def\Newton{\mathrm{N}}
\def\Pascal{\mathrm{Pa}}
\def\NewtonVsSquareMillimeter{\frac{\mathrm{N}}{\mathrm{mm}^2}}
\def\NewtonVsMeter{\frac{\mathrm{N}}{\mathrm{m}}}

% Title
% \usepackage{tikz}
\usepackage{lipsum} % für Blindtext

% Farben
\usepackage{xcolor}
\usepackage{lmodern}
\usepackage{fancyhdr}
\usepackage{lastpage}

% --- Farben ---
\definecolor{mainblue}{RGB}{0,70,127}
\definecolor{lightgray}{gray}{0.9}
\definecolor{footergray}{gray}{0.4}

% --- Fancyhdr: Fußzeile für alle Seiten außer Titelseite ---
\fancypagestyle{main}{
  \fancyhf{}
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
  \fancyhead[L]{Beam Equation and PINN}
  \fancyhead[R]{\thepage}
  % \fancyfoot[L]{\textcolor{footergray}{Robert Haas – Technical Software and Mathematics}}
  % \fancyfoot[C]{\textcolor{footergray}{Page \thepage\ of \pageref{LastPage}}}
}

\begin{document}


% --- Titelseite ---
\begin{titlepage}
    \centering

    % Zentrierte blaue Linie oben
    \vspace*{1cm}
    {\color{mainblue} \rule{1.0\textwidth}{2pt}}\\[0.5cm]

    % Titel
    {\Huge\bfseries The Euler-Bernoulli Beam Equation\\[0.3em]
    and Physics Informed Neural Networks (PINN)\par}
    
    \vspace*{0.5cm}
    {\color{mainblue} \rule{1.0\textwidth}{2pt}}\\[1.5cm]
    
    \vspace{1cm}

    % Autor
    {\Large Robert Haas}\\[0.3em]
    % {\normalsize Dr. Robert Haas Technical Software and Mathematics}\\[0.5cm]

    % Grauer Infoblock
    \vspace{4cm}
    \colorbox{lightgray}{
        \parbox{0.8\textwidth}{
            \vspace{1em}
            \centering
            {\normalsize\textbf{Documentation} • \today{ }• Version 1.0}\\[0.5em]
            {\small PDF generated with \LaTeX}
            \vspace{1em}
        }
    }

    \vfill

    % Projektinfo (Fußbereich)
    {\footnotesize
    \begin{tabular}{rl}
    \textbf{Project page:} & \href{https://github.com/Haasrobertgmxnet/BeamEquation}{https://github.com/Haasrobertgmxnet/BeamEquation}\\
    \textbf{Contact:} & \href{mailto:Haasrobert@gmx.net}{Haasrobert@gmx.net} \\
    \end{tabular}
    }

    \vspace{2cm}
    {\color{mainblue} \rule{1.0\textwidth}{2pt}}  % Zentrierte Linie unten
\end{titlepage}

\pagestyle{main}

\begin{abstract}
A numerical study for the 1D Euler-Bernoulli equation for a deflected beam is done here. The Euler-Bernoulli equation will be presented in different forms which are used in the calculations afterwards. For the calculations of the Euler-Bernoulli equation \emph{physics informed neural networks (PINN)} have been used. \emph{First}, the equation itself has been solved using a PINN. The result has been compared with a classical finite element (FE) solution. Since FE solvers are optimized for classical linear engineering problems, the FE approach performs far better than the PINN. \emph{Second}, simulated deflection measurements are used for approximating a solution that provides the data to estimate the unknown elastic modulus. This is done with two PINNs . This is a typical data-driven use case for using PINNs and here very satisfactory results can be obtained by a careful adjustment of the calculation parameters.
\end{abstract}

\tableofcontents
\newpage

\section{The Euler-Bernoulli Equation of a Beam}

For studying the defection behavior of a beam we consider a beam with the following assumptions:
\begin{enumerate}
\item[(A1)] The beam under consideration is a cantilever beam: At one end the beam is fixed in that way, that there is no displacement and no rotation allowed. At the other end -the free end- displacement and rotation are allowed to occur.
\item[(A2)] The beam has a fixed constant cross-section.
\item[(A3)] Hooke's law, i.e. the stress $\sigma $ is proportional to the strain $\epsilon$, is fulfilled everywhere.
\end{enumerate}

The Euler-Bernoulli equation is given by
\begin{equation} \label{PrincipalEulerBernoulli}
\frac{\mathrm{d}^2}{\mathrm{d}x^2}\left( E (x) \cdot I \cdot \frac{\mathrm{d}^2 w(x)}{\mathrm{d}x^2} \right) = q \text{ for } x \in (0,L) .\\
\end{equation}
With constant elastic modulus $E$ this simplifies to
\begin{equation} \label{PrincipalEulerBernoulliConstantE}
E\cdot I \cdot \frac{\mathrm{d}^4 w(x)}{\mathrm{d}x^4} = q \text{ for } x \in (0,L)\\
\end{equation}
subject to the boundary conditions
\begin{equation} \label{PrincipalBoundaryConditions}
w( 0) = \frac{\mathrm{d} w}{\mathrm{d}x}(0) = 0 \text{ , and } \frac{\mathrm{d}^2 w}{\mathrm{d}x^2}(L) = \frac{\mathrm{d}^3 w}{\mathrm{d}x^3}(L) = 0.
\end{equation}
The physical quantities in equation (\ref{PrincipalEulerBernoulli}) are described in the following table.

\begin{table}[h!] 
\centering
% \renewcommand{\arraystretch}{1.3}
\begin{tabular}{cclc}
\toprule
\textbf{Symbol} & \textbf{SI Unit} & \textbf{Description}& \textbf{Typical Value}\\
\midrule
$L$ & $\Meter$ & Length & $5\; \Meter$\\
$w$ & $\Meter$ & Deflection &  $w<< L $\\
$E$ & $\Pascal$ & Elastic modulus (Young's modulus) & $210\;000\; \NewtonVsSquareMillimeter$\\
$I$ &$ \Meter^4$ & Second moment of area & $10^{-6}\;\Meter^4$ \\
$q$ &$ \NewtonVsMeter $ & Distributed load & $1000 \;\NewtonVsMeter $ \\
\bottomrule
\end{tabular}
\caption{Physical quantities.}
\label{TableTypicalValues}
\end{table}

Unless otherwise stated we consider the Euler-Bernoulli equation (\ref{PrincipalEulerBernoulliConstantE}), i.e. the variant with constant elastic modulus $E$. For this case an exact solution is given by
\begin{equation*}
w(x) = \frac{q}{24 E\cdot I }\cdot x^2 \cdot \left( x^2 -4L\cdot x + 6L^2\right) .
\end{equation*}
With the values given in Table \ref{TableTypicalValues} we get
\begin{equation} \label{MaximalDeflection}
w( L) = \frac{q}{24 E\cdot I }\cdot L^2 \cdot \left( L^2 -4L\cdot L + 6L^2\right) = \frac{q\cdot L^4}{8 E\cdot I } \approx 0.37202 \text{ m},
\end{equation}
which is the deflection of the beam at position $x = L$.

Following \cite[pp. 3]{hoffmann2014modellierung} a dimensionless form of the equation (\ref{PrincipalEulerBernoulliConstantE}) can be derived to give
\begin{equation} \label{DimensionlessEulerBernoulliConstantE}
% \frac{\mathrm{d}^2}{\mathrm{d}\xi^2}\left( e(\xi ) \frac{\mathrm{d}^2 v(\xi)}{\mathrm{d}\xi^2} \right)  = 1 \text{ or } 
\frac{\mathrm{d}^4 v(\xi)}{\mathrm{d}\xi^4} = 1 \text{ for } \xi \in (0,1),\\
\end{equation}
where
\begin{eqnarray*}
E (x) &=& E_{\mathrm{char}} \cdot e(\xi ) , \\
w(x) &=& w_{\mathrm{char}} \cdot v(\xi ) , \\
x &=& \xi\cdot L, \\
w_{\mathrm{char}} & = & \frac{q\cdot L^4}{E\cdot I }, \\
\frac{\mathrm{d}^4 w(x)}{\mathrm{d}x^4} & = & \frac{w_{\mathrm{char}}}{L^4}\cdot \frac{\mathrm{d}^4 v(\xi)}{\mathrm{d}\xi^4}.
\end{eqnarray*}
Here $E_{\mathrm{char}} $ may be chosen as 
\begin{equation*}
E_{\mathrm{char}} = \mathrm{max}_{x\in \lbrack 0,L \rbrack } E(x) .
\end{equation*}
The boundary conditions now read as
\begin{equation} \label{DimensionlessBoundaryConditions}
v( 0) = \frac{\mathrm{d} v}{\mathrm{d}\xi }(0) = 0 \text{ , and } \frac{\mathrm{d}^2 v}{\mathrm{d}\xi^2}(1) = \frac{\mathrm{d}^3 v}{\mathrm{d}\xi^3}(1) = 0.
\end{equation}
Again, there exists an exact solution, given by
\begin{equation*} 
v(\xi ) = \frac{1}{24} \cdot \xi^2 \cdot \left( \xi^2 -4 \xi + 6 \right) .
\end{equation*}
The equation (\ref{DimensionlessEulerBernoulliConstantE}) is crucial for the application of some numerical methods, e.g. physics informed neural networks (PINN).


\subsection*{The Finite-Element Method (FEM) for solving the Euler-Bernoulli Equation}
The FEM is set up for the the dimensionless problem (\ref{DimensionlessEulerBernoulliConstantE}) and the boundary conditions (\ref{DimensionlessBoundaryConditions}). Here, classical 1D beam elements (third order Hermite polynomials) on 99 elements are used to solve this equation. The theory on FEM to solve the Euler-Bernoulli equation can be found in any good text book.
The results of the calculation will be shortly presented in Table \ref{ComparisonFwdProblem}.

\begin{comment}
\[
\mathbf{k}_e = \frac{EI}{L^3}
\begin{bmatrix}
12 & 6L & -12 & 6L \\
6L & 4L^2 & -6L & 2L^2 \\
-12 & -6L & 12 & -6L \\
6L & 2L^2 & -6L & 4L^2 \\
\end{bmatrix}
\]

\begin{equation*}
\mathbf{f}_e = \frac{q L}{2}
\begin{bmatrix}
1 \\
\frac{L}{6} \\
1 \\
-\frac{L}{6} \\
\end{bmatrix}
\end{equation*}
\end{comment}

\section{The Idea of a Physics Informed Neural Network (PINN)}

Neural Networks are usually used for either for \emph{classification} or \emph{approximation}. A short introduction to neural networks can be found in \cite{rojas1993neuronale} and \cite[pp. 809]{stoecker1995formeln} for example. Today the main applications of neural networks contain classification. On the other hand, neural networks can be used to approximate an (unknown) function $v$. If this function $v$ arises from a physics or engineering context, i.e. from a mathematical equation of  physics the equation structure will be incorporated into the neural network or the learning algorithm in some way. In this case the neural network is often called \emph{phyiscs informed neural networks (PINN)}. Here the key task is to construct an approximation $ \hat{v} $ for the unknown $v$. Assuming there is a PINN of $n$ layers then the output $O^{(k)} $of layer $k$ is connected to the output $O^{(k-1)} $ of layer $k-1$ via
\begin{equation*}
O^{(k)}_i = f^{(k-1)}\left( b^{(k-1)}_i + \sum_j \theta^{(k-1)}_{i,j} O^{(k-1)}_j \right),
\end{equation*}
where $ O^{(k-1)}_j $ is  the output of the $j$-th node in layer $k-1$, $ \theta^{(k-1)}_{i,j} $ is the weight of the connection between  the $j$-th node in layer $k-1$ and the $i$-th node in layer $k$, $ b^{(k-1)}_i $ is the bias neuron of $i$-th node in layer $k-1$ and $f^{(k-1)}$ is the activation of the nodes of layer $k-1$.

For a function $v$ of one variable and a PINN with only one hidden layer and $f^{(2)} = 1 $ the approximation task is to find weights $\theta $ and a bias neuron $b$ such that
\begin{equation*}
\hat{v} (\xi) =   b_1^{(2)} + \sum_j \theta^{(2)}_j f^{(1)}\left( b_j^{(1)} + \theta_1^{(1)} \xi \right).
\end{equation*}
This illustrates that the unknown function $v$ is essentially approximated by a linear combination of the activation functions whereas the numbers $b_j^{(1)}$, $ b_1^{(2)} $, $ \theta_1^{(1)} $ and $ \theta^{(2)}_j $ are adjusted by the backpropagation algorithm to minimise $ \sum_{j=1}^N\| \hat{v} (\xi_j) - v(\xi_j) \|^2 $, for example.

There are two important application cases where PINNs are applied to:
\begin{itemize}
\item Solving a set of mathematical equations. This is called the \emph{direct problem}. The set of mathematical equations is usually a set of differential equations and their boundary conditions. These differential equations can be either ordinary or partial differential equations.
\item Estimation of an unknown parameter in the set of mathematical equations where its solution is given by simulated measured values for example. This is called the \emph{inverse problem}.
\end{itemize}

Typically neural networks learn by minimising a loss function. A key concept in PINN is to extend the loss function by a term that penalizes deviations from the governing equations and the boundary conditions.

Mathematical equations in engineering or physics are often of the form
\begin{eqnarray*}
\mathscr{A} (v) &=& 0,\text{ in the domain,} \\
\mathscr{B} (v) &=& 0,\text{ on the boundary. }
\end{eqnarray*}
In most cases, $ \mathscr{A} $ is a differential operator either for an ODE or a PDE. Then, $ \mathscr{B} $ is the boundary operator. For the direct problem the function typically consists of
\begin{eqnarray} \label{LossTerm1}
\text{Physics loss } &=& \mathscr{L}_{\mathrm{Physics}} (\hat{v}) =\beta \sum_{k = 1}^N\| \mathscr{A} (\hat{v}(\xi_k) ) \|^2 \\
\text{Boundary loss} &=& \mathscr{L}_{\mathrm{Boundary}} (\hat{v}) = \beta \sum_{k = 1}^M\| \mathscr{B} (\hat{v}(\xi_{n_k}) ) \|^2 \text{ with boundary points  } \xi_{n_k} \label{LossTerm2}\\
\text{Regularisation term} &=& \beta \sum_{\theta \in \mathrm{PINN Weights}}  \| \theta \|^2 . \label{LossTerm3}
\end{eqnarray}
Often, neural network libraries use the mean square error as loss function, then $\beta = 1 /N  $ i.e. the reciprocal of the number of sample points. Otherwise, if the square of the Euclidean distance is used, then $\beta $ is equal to one.
For the inverse problem the PINN approximation an additional loss term,
\begin{equation*}
\text{Data loss} = \mathscr{L}_{\mathrm{Data}} (\hat{v})  = \beta \sum_{k = 1}^N \| \hat{v}(\xi_k)  - v_k \|^2 \\,
\end{equation*}
will be added to the terms (\ref{LossTerm1}),  (\ref{LossTerm2}) and (\ref{LossTerm3}). Here $ v_k $ are the simulated measured values at position $\xi_k $.

\section{PINNs for solving the Euler-Bernoulli Equation}
In case of the equation (\ref{PrincipalEulerBernoulliConstantE}) the term would read $\| E\cdot I \cdot \mathrm{d}^4 w(x) / \mathrm{d}x^4 - q \|^2 $. Since neural networks are sensitive to unscaled input data the dimensionless version (\ref{DimensionlessEulerBernoulliConstantE}) is used to give 

\begin{equation*}
\text{Physics loss} = \sum_{\xi \in \{\text{Sample points}\}} \left\| \tfrac{\mathrm{d}^4 v(\xi)}{\mathrm{d}\xi^4} -1 \right\|^2 + \text{Loss from boundary conditions}.
\end{equation*}

The loss from the boundary conditions (\ref{DimensionlessBoundaryConditions}) is modeled as follows

\begin{equation*}
\text{Boundary loss} = \| v (0) \|^2 + \left\|\tfrac{\mathrm{d} v}{\mathrm{d}\xi}(0) \right\|^2 + \left\|\tfrac{\mathrm{d}^2 v}{\mathrm{d}\xi^2} (1) \right\|^2 + \left\|\tfrac{\mathrm{d}^3 v}{\mathrm{d}\xi^3}(1) \right\|^2 .
\end{equation*}

The total loss is then
\begin{eqnarray} \label{LossDirectProblem}
\text{Total loss} & = & \text{Physics loss} + \text{Boundary loss} + \text{Regularisation term} \\
 & = & \sum_{\xi \in \{\text{Sample points}\}} \left\| \tfrac{\mathrm{d}^4 v(\xi)}{\mathrm{d}\xi^4} -1 \right\|^2 + \| v (0) \|^2 + \left\|\tfrac{\mathrm{d} v}{\mathrm{d}\xi}(0) \right\|^2 + \left\|\tfrac{\mathrm{d}^2 v}{\mathrm{d}\xi^2} (1) \right\|^2 + \left\|\tfrac{\mathrm{d}^3 v}{\mathrm{d}\xi^3}(1) \right\|^2 \\ 
 & + & \lambda \sum_{\theta \in \{\text{PINN weights}\}} \| \theta \|^2 .
\end{eqnarray}

\subsection*{The Setup of the PINN}
Here we set up a feed forward neural network of the following structure:
\begin{table}[h!]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Layer} & \textbf{Nodes} & \textbf{Description}\\
\midrule
Input Layer & 1 & The input variable $\xi $ sampled at 100 points between 0 and 1\\
Hidden Layer & 5 & Classical hidden layer with tanh activation\\
Output Layer & 1 & The output variable $v(\xi ) $ at the sample points\\
\bottomrule
\end{tabular}
\caption{Architecture of the PINN.}
\end{table}

Two solving methods are executed sequentially, i.e.
\begin{enumerate}
\item Adaptive Moment estimation (ADAM), and
\item Limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS), afterwards. That is for refinement of the solution.
\end{enumerate}
To train the PINN a set of 100 equidistantly distributed sample points on the interval $\lbrack 0, 1 \rbrack $ (including the interval boundaries) is used to propagate forward through the PINN. The setup of the parameters of the ADAM and LBFGS solver is as follows.

\begin{table}[htbp]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Property Name} & \textbf{Property Value}\\
\midrule
Learning rate & 0.1\\
Epochs & 200 \\
Loss Function & Mean square error (MSE) \\
Early stopping & at threshold $10^{-6} $ \\
$L^2$-Regularisation & switched off by default \\
\bottomrule
\end{tabular}
\caption{ADAM setup.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Property Name} & \textbf{Property Value}\\
\midrule
Learning rate & 0.01\\
Iterations & 300 \\
Loss Function & Mean square error (MSE) \\
Stopping & $10^{-5} $ for the gradients, $10^{-9} $ for the MSE\\
$L^2$-Regularisation & switched off by default \\
\bottomrule
\end{tabular}
\caption{LBFGS setup.}
\end{table}

\subsection*{PINN Results}
The Euler-Bernoulli equation has solved using the PINN as configured in the section before. The errors -MSE and relative- are moderate and acceptable in some way. Nevertheless, compared to the finite element solution, the errors of the FE method are far below the ones of the PINN method. That confirms just the fact that FEM was designed to solve linear elasticity problems and hence FEM is a very powerful tool tackling such problems. After discussing the results of ODE solving we present a problem where PINN is very useful to use.

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Execution Time (s)} & \textbf{RMSE} & \textbf{Relative Error} \\
\midrule
ADAM & 3.2240 & 8.401149e-02 & 4.435198e-01 \\
LBFGS & 2.7095 & 9.454326e-06 & 4.991199e-05 \\
FEM  & 0.1740   & 1.863023e-08 & 9.835413e-08 \\
\bottomrule
\end{tabular}
\caption{Execution time, RMSE values and relative errors wrt exact solution.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ccccc}
\toprule
\textbf{Method} & \textbf{ADAM} & \textbf{LBFGS} & \textbf{FEM} & \textbf{Exact} \\
\midrule
\textbf{ADAM} & - & 8.400263e-02 & 8.401147e-02 & 8.401149e-02 \\
\textbf{LBFGS}  & 4.434940e-01 & -  & 9.443193e-06 & 9.454326e-06 \\
\textbf{FEM} & 4.435197e-01 & 4.985322e-05 & - & 1.863023e-08 \\
\textbf{Exact} & 4.435198e-01 & 4.991199e-05 & 9.835413e-08 & - \\
\bottomrule
\end{tabular}
\caption{Relative errors (lower left) and RMSE values (upper right).}
\label{ComparisonFwdProblem}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Charts/Exact_Solution.png}
    \caption{PINN Solution vs exact Solution.}
    \label{fig:example}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Charts/FEM_Solution.png}
    \caption{PINN Solution vs FEM Solution.}
    \label{fig:example}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Charts/Error_Adam.png}
    \caption{The error of the ADAM calculation vs time/epochs.}
    \label{fig:example}
\end{figure}

\section{Parameter Identification for variable Elastic Modulus}

\subsection*{Statement of the Parameter Identification Problem}
We now assume that the elastic modulus is an unknown material property due inhomogeneities of the material. Such an inhomogeneity could be rust in an iron beam, for example. On the other hand we assume that the deflection of the beam is known in form of simulated measurements. For this purpose we turn back to the form (\ref{PrincipalEulerBernoulli}) of the Euler-Bernoulli equation with variable $E = E(x) $:
$$
\frac{\mathrm{d}^2}{\mathrm{d}x^2} \left( E(x)\cdot I \cdot \frac{\mathrm{d}^2 w(x)}{\mathrm{d}x^2} \right) = q(x) \text{ for } x \in (0,L),\\
$$
with boundary conditions (\ref{PrincipalBoundaryConditions}) as before. For the numerical solution we use the dimensionless form
\begin{equation} \label{DimensionlessEulerBernoulli}
\frac{\mathrm{d}^2}{\mathrm{d}\xi^2}\left( e(\xi ) \frac{\mathrm{d}^2 v(\xi)}{\mathrm{d}\xi^2} \right)  = 1 \text{ for } \xi \in (0,1),\\
\end{equation}
subject to the boundary conditions (\ref{DimensionlessBoundaryConditions}) where
\begin{equation*}
e(\xi ) = \frac{E( x ) }{E_{\mathrm{char}}}\text{ and } E_{\mathrm{char}} = \mathrm{max}_{x\in \lbrack 0, L \rbrack } E(x) .
\end{equation*}

\subsection*{Setup of the PINN and the Solvers}

Following \cite{teloli2024solving} two neural networks are to be trained now:
\begin{itemize}
\item
A neural network $\mathscr{V}$ to get an approximation $\hat{v} (\xi ) $ for the simulated measurements of $w$.
\item
A neural network $\mathscr{E}$ to get an estimation $\hat{e} (\xi) $ for the unknown elastic modulus.
\end{itemize}
These two neural networks will be trained simultaneously. Their architecture is given by the following two tables.
\begin{table}[h!]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Layer} & \textbf{Nodes} & \textbf{Description}\\
\midrule
Input Layer & 1 & The input variable $\xi $ sampled at 100 points between 0 and 1\\
Hidden Layer 1 & 20 & Classical hidden layer with tanh activation\\
Hidden Layer  2& 20 & Classical hidden layer with tanh activation\\
Output Layer & 1 & The output variable $v(\xi ) $ at the sample points\\
\bottomrule
\end{tabular}
\caption{Architecture of $\mathscr{V}$.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Layer} & \textbf{Nodes} & \textbf{Description}\\
\midrule
Input Layer & 1 & The input variable $\xi $ sampled at 100 points between 0 and 1\\
Hidden Layer 1 & 35 & Classical hidden layer with tanh activation\\
Hidden Layer  2& 35 & Classical hidden layer with tanh activation\\
Output Layer & 1 & The output variable $e(\xi ) $ at the sample points\\
\bottomrule
\end{tabular}
\caption{Architecture of $\mathscr{E}$.}
\end{table}
Both networks will be trained using a common loss function. This loss function has a rather complicated structure than (\ref{LossDirectProblem}) for the direct problem:
\begin{eqnarray*}
\text{Total loss} & = &  \alpha_1 \cdot \text{Data loss} + \alpha_2 \cdot \text{Physics loss} + \alpha_3 \cdot\text{Boundary loss} \\
& + &  \alpha_4 \cdot \text{Loss from gradient terms of } E +\alpha_5 \cdot\text{Regularisation term}
\end{eqnarray*}
where
\begin{eqnarray*}
\text{Data loss} & = & \sum_{\xi \in \{\text{Sample points}\}} \left\| \hat{v}(\xi) - v_{\mathrm{measure}} (\xi ) \right\|^2 \\
\text{Physics loss} & = & \sum_{\xi \in \{\text{Sample points}\}} \left\|  \tfrac{\mathrm{d}^2}{\mathrm{d}\xi^2} \left(  \tfrac{\mathrm{d}^2 v(\xi)}{\mathrm{d}\xi^2} \right)-1  \right\|^2 \\
\text{Boundary loss}  & = & \| v (0) \|^2 + \left\|\tfrac{\mathrm{d} v}{\mathrm{d}\xi}(0) \right\|^2 + \left\|\tfrac{\mathrm{d}^2 v}{\mathrm{d}\xi^2} (1) \right\|^2 + \left\|\tfrac{\mathrm{d}^3 v}{\mathrm{d}\xi^3}(1) \right\|^2 \\
\text{Loss from gradient terms of } E  & = & \sum_{\xi \in \{\text{Sample points}\}} \left\|  \tfrac{\mathrm{d} e ( \xi ) }{\mathrm{d}\xi} \right\|^2 \\
\text{Regularisation term} & = & \sum_{\theta \in \{\text{weights of }\mathscr{E}\}} \| \theta \|^2 .
\end{eqnarray*}
For the parameters $ \alpha_k \; k\in\{1,2,3,4,5\} $ the choices are made as in Table ref{LossFunctionAdjustment}.
\begin{table}[h!]
\centering
\begin{tabular}{cl}
\toprule
\textbf{Parameter} & \textbf{Value}\\
\midrule
$ \alpha_1 $ & 100.0 \\
$ \alpha_2 $ & 1.5 \\
$ \alpha_3 $ & 2.0 \\
$ \alpha_4 $ & 1e-5 \\
$ \alpha_5 $ & 1e-3 \\
\bottomrule
\end{tabular}
\caption{Adjustment values for the loss function.}
\label{LossFunctionAdjustment}
\end{table}

Again, a combination of an ADAM solver and a LBFGS solver has been used. The adjustments are given in the tables \ref{AdamSetup2} and \ref{LBFGSSetup2}.

\begin{table}[htbp]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Property Name} & \textbf{Property Value}\\
\midrule
Learning rate & 0.005\\
Epochs & 3000 \\
Loss Function & Mean square error (MSE) \\
Early stopping & at threshold $10^{-6} $ \\
\bottomrule
\end{tabular}
\caption{ADAM setup.}
\label{AdamSetup2}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Property Name} & \textbf{Property Value}\\
\midrule
Learning rate & 0.005\\
Iterations & 1500 \\
Loss Function & Mean square error (MSE) \\
Stopping & $10^{-5} $ for the gradients, $10^{-9} $ for the MSE\\
\bottomrule
\end{tabular}
\caption{LBFGS setup.}
\label{LBFGSSetup2}
\end{table}

\subsection*{Results of the PINN Training}

With these values the two PINNs $\mathscr{V}$ and $\mathscr{E} $ have been trained using Python and PyTorch. Therefore 100 pairs of random integers - each uniformly distributed between 0 and 100- are generated. Then the first random number of each pair is used to generate a normally distributed noise with zero mean and standard deviation of 0.002506 to be added to the exact solution $v(\xi ) $. This will simulate measured data which randomly vary around the exact solution. The second random number sets the seed for the randomly generated initial weights of the two neural networks $\mathscr{V}$ and $\mathscr{E} $.
For 99 out of the 100 samples we have the following 
\begin{table}[h!]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Loss} & \textbf{Value}\\
\midrule
Data loss & < 9.27e-6\\
Physics loss & < 1.42e-5\\
Boundary loss & < 1.36e-5 \\
Max difference in $E $ & < 16385 Pa \\
R² value & > 0.993 \\
\bottomrule
\end{tabular}
\caption{Results for 99 of 100 test samples.}
\end{table}
These results show a very near and acceptable approximation.

There is one pathological case where the approximation seriously fails:
\begin{table}[h!]
\centering
\begin{tabular}{ccl}
\toprule
\textbf{Loss} & \textbf{Value}\\
\midrule
Data loss & 9.90e-5\\
Physics loss & 1.095e-4 \\
Boundary loss &  1.640e-2 \\
Max difference in $E $ & 1.574e11 Pa \\
R² value &  0.937 \\
\bottomrule
\end{tabular}
\caption{Results for the pathological case.}
\end{table}
The poor approximation of this case may be easily seen in the following diagram:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Charts/Dataset with minimal R2Deflection_PINN.png}
    \caption{Pathologically estimated $w$ vs simulated measurements.}
\end{figure}

Beyond the wrong shape the boundary conditions are obviously not fulfilled. The following chart which shows the exact solution $w$ versus the measured values seems to say that the measured values are not the reason for the poor approximation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Charts/Dataset with minimal R2Deflection_exact.png}
    \caption{Exact $w$ and simulated measurements. The estimations for $w$ in the 99 good cases will be visually the same as the exact $w$, since the MSE is below 1e-5.}
\end{figure}

Two of the 99 good samples have the same random seed as the pathological sample and thus the same generated measured values. It follows that these simulated measurements are not the reason for the poor approximation quality. Furthermore, choosing another random initial weights for the PINN training will solve the problem. Choosing another solver than ADAM or skipping ADAM to train only with LBFGS could be another solution.

%%
\begin{comment}
\section{Program Code}
\subsection*{Code for Solving the Euler-Bernoulli Equation}
Algorithms or code snippets can be added easily:

%\lstinputlisting[style=mypython, caption={Beam.Python.py}]{C:/Users/haasr/source/repos/BeamEquation/Beam.Python/Beam.Python.py}

%\lstinputlisting[style=mypython, caption={BeamPINN.py}]{C:/Users/haasr/source/repos/BeamEquation/Beam.Python/Beam_PINN.py}

%\lstinputlisting[style=mypython, caption={BeamFEM.py}]{C:/Users/haasr/source/repos/BeamEquation/Beam.Python/Beam_FEM.py}

\subsection*{PINN for the Parameter Identification of Elastic modulus}
%\lstinputlisting[style=mypython, caption={ParameterIdentification.Python.py}]{C:/Users/haasr/source/repos/BeamEquation/ParameterIdentification.Python/ParameterIdentification.Python.py}

%\lstinputlisting[style=mypython, caption={BeamPINNinverse.py}]{C:/Users/haasr/source/repos/BeamEquation/ParameterIdentification.Python/Beam_PINN_inverse.py}
%\section{Discussion}
Detailed analysis and discussion of the results.
\end{comment}
\section{Conclusion and Outlook}
This short case study using PINNs for calculating solutions and parameters of the 1D Euler-Bernoulli equation leads to the following conclusions:
\begin{enumerate}
\item
Calculating a solution of the Euler-Bernoulli equation using classical finite elements is much faster and more precise than using a PINN. Of course finite element methods have been just developed to solve linear mechanical problems and thus they are optimized for that purpose. The nonlinear and nested structure of  a neural network may be not adequate or optimal for that purpose.
\item
Estimating the elastic modulus from simulated deflection values is a rather data-driven problem and the PINN approach fits far better there. With a careful adjustment of the network architecture, the solver adjustments and the loss contributions it was possible to calculate results with very satisfactory precision in case of 99 samples.
\item
For one single sample the estimation of elastic modulus failed producing results with poor accuracy. The reason is an appropriate choice of initial weights of the PINN and can be solved using another initial set up.
\item
The very fine and careful setup of the network architecture, the solver settings and the loss balancing may be improved by varying these parameters in a systematic way. It is desirable to reduce the number of epochs without loosing precision. This probably can be achieved by another balancing of the loss contributions, a modification of the PINN architecture and better settings for the solver.
\item
The calculations have been done in Python using Pytorch. To reduce the calculation time an implementation in C++ would be more adequate. One of the most performant  C++ frameworks is OpenNN which is far faster than Tensorflow or Pytorch. Unfortunately, OpenNN does not really support the use of PINNs, especially there is no automatic differentiation and no possibility to access the loss function of the hidden layers via the OpenNN API. The other C++ Machine Learning framework, MLPack, has similar difficulties with PINN and does not support PINNs very well.
\item
A way to implement a PINN study in C++ could be the use of the Pytorch C++ API. That may be as not as fast as OpenNN or MLPack but faster than any Python implementation.
\end{enumerate}


\newpage
\begin{thebibliography}{9}

% \bibitem{duque2015beam}
% D.~Duque, \emph{A derivation of the beam equation}, arXiv preprint arXiv:1512.01171, 2015. [Online]. Available: \url{https://arxiv.org/abs/1512.01171}

\begin{comment}
\bibitem{gross}
D.~Gross, W.~Hauger, J.~Schröder and W.~A. Wall,
\emph{Technische Mechanik 2: Elastostatik: Band 2: Elastostatik}
10th rev.ed., Springer-Verlag Berlin Heidelberg, 2009.
\end{comment}

\bibitem{hoffmann2014modellierung}
K.-H. Hoffmann and G. Witterstein, 
\emph{Mathematische Modellierung: Grundprinzipien in Natur- und Ingenieurwissenschaften}, 
Mathematik Kompakt, Birkhäuser Basel, 1st ed. (corrected reprint), 2014.

\begin{comment}
\bibitem{raissi2019physics}
M.~Raissi, P.~Perdikaris, and G.~E. Karniadakis, 
\emph{Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations}, 
Journal of Computational Physics, vol. 378, pp. 686–707, 2019. [Online]. Available: \url{https://arxiv.org/pdf/1907.04502}
\end{comment}

\bibitem{rojas1993neuronale}
R.~Rojas,
\emph{Theorie der neuronalen Netze: Eine systematische Einführung},
1st ed. (corrected reprint 1996), Springer-Lehrbuch, Springer-Verlag Berlin, 1993.

\bibitem{stoecker1995formeln}
H.~Stöcker, 
\emph{Taschenbuch mathematischer Formeln und moderner Verfahren}, 
3rd rev. and ext. ed., Harri Deutsch Verlag, Frankfurt am Main, 1995.

\bibitem{teloli2024solving}
C.~Teloli, A.~Titarelli, A.~Guerrieri, and S.~Vidoli, 
\emph{Solving linear beam dynamics with neural operators: A comprehensive study}, 
Mechanics of Materials, vol. 190, 104226, 2024. [Online]. Available: \url{https://www.sciencedirect.com/science/article/pii/S0888327024010884}

\begin{comment}
\bibitem{timoshenko1951theory}
S.~P. Timoshenko and J.~M. Gere, 
\emph{Theory of Elastic Stability}, 
2nd ed., McGraw-Hill, New York, 1961.
\end{comment}

\bibitem{code}
Code samples used in this work: \url{https://github.com/Haasrobertgmxnet/BeamEquation}

\end{thebibliography}


\newpage

% \printbibliography

\end{document}
